# -*- coding: utf-8 -*-
"""Copy of RLT - 5 MDP .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SUi-vbXfKPjQQy94DuAJdi6S2KcPg2-e
"""

class MDP:
    def __init__(self, states, actions, transitions, rewards, gamma):
        self.states = states
        self.actions = actions
        self.transitions = transitions
        self.rewards = rewards
        self.gamma = gamma

def value_iteration(mdp, epsilon=0.01, max_iter=1000):
    V = {s: 0 for s in mdp.states}
    for _ in range(max_iter):
        delta = 0
        for s in mdp.states:
            v = V[s]
            V[s] = max([sum([p * (r + mdp.gamma * V[s1]) for (p, s1, r) in
                             mdp.transitions(s, a)]) for a in mdp.actions])
            delta = max(delta, abs(v - V[s]))
        if delta < epsilon:
            break
    return V

states = [(0, 0), (0, 1), (1, 0), (1, 1)]
actions = ['up', 'down', 'left', 'right']
gamma = 0.9

def transitions(state, action):
    if action == 'up':
        next_states = [(0.8, (state[0], state[1]), -1)]
        if state[0] > 0:
            next_states.append((0.1, (state[0] - 1, state[1]), -1))
        if state[1] < 1:
            next_states.append((0.1, (state[0], state[1] + 1), -1))
        return next_states
    elif action == 'down':
        next_states = [(0.8, (state[0], state[1]), -1)]
        if state[0] < 1:
            next_states.append((0.1, (state[0] + 1, state[1]), -1))
        if state[1] > 0:
            next_states.append((0.1, (state[0], state[1] - 1), -1))
        return next_states
    elif action == 'left':
        next_states = [(0.8, (state[0], state[1]), -1)]
        if state[1] > 0:
            next_states.append((0.1, (state[0], state[1] - 1), -1))
        if state[0] > 0:
            next_states.append((0.1, (state[0] - 1, state[1]), -1))
        return next_states
    elif action == 'right':
        next_states = [(0.8, (state[0], state[1]), -1)]
        if state[1] < 1:
            next_states.append((0.1, (state[0], state[1] + 1), -1))
        if state[0] < 1:
            next_states.append((0.1, (state[0] + 1, state[1]), -1))
        return next_states
    else:
        return []

def rewards(state, action):
    return 0

mdp = MDP(states, actions, transitions, rewards, gamma)

V = value_iteration(mdp)

print("Optimal Value Function:")
for state, value in V.items():
    print(f"State {state}: Value {value}")