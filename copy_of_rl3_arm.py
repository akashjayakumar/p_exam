# -*- coding: utf-8 -*-
"""Copy of RL3 Arm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/150pgRKU4yw4ISRNnH0JibojOBkBoCw-K
"""

import numpy as np
import matplotlib.pyplot as plt

class MultiArmedBandit:
    def __init__(self, k, epsilon, true_reward=0):
        """
        Initializes the Multi-Armed Bandit.
        :param k: Number of arms.
        :param epsilon: Exploration rate.
        :param true_reward: True average reward for each arm (for simulation).
        """
        self.k = k
        self.epsilon = epsilon
        self.true_reward = true_reward
        self.initialize()
    def initialize(self):
        """
        Resets the bandit for a new run.
        """
        self.q_true = np.random.randn(self.k) + self.true_reward  # True values for each arm
        self.q_estimated = np.zeros(self.k)  # Estimated values for each arm
        self.action_count = np.zeros(self.k)  # Count of selections for each arm
        self.best_action = np.argmax(self.q_true)  # Best action based on true values

    def act(self):
        """
        Chooses an action based on the ε-greedy strategy.
        """
        if np.random.rand() < self.epsilon:
            return np.random.choice(np.arange(self.k))  # Explore
        else:
            return np.argmax(self.q_estimated)  # Exploit

    def step(self, action):
        """
        Takes an action and returns the reward.
        :param action: Action chosen.
        """
        # Simulate reward based on true value with some noise
        reward = np.random.randn() + self.q_true[action]
        self.action_count[action] += 1
        # Update estimated value with incremental formula
        self.q_estimated[action] += (reward - self.q_estimated[action]) / self.action_count[action]
        return reward

def simulate(runs, time, bandits):
    """
    Runs the simulation.
    :param runs: Number of runs of the experiment.
    :param time: Number of steps in each run.
    :param bandits: List of bandit problems.
    """
    rewards = np.zeros((len(bandits), runs, time))
    best_action_counts = np.zeros(rewards.shape)

    for i, bandit in enumerate(bandits):
        for r in range(runs):
            bandit.initialize()
            for t in range(time):
                action = bandit.act()
                reward = bandit.step(action)
                rewards[i, r, t] = reward
                if action == bandit.best_action:
                    best_action_counts[i, r, t] = 1

    mean_rewards = rewards.mean(axis=1)
    mean_best_action_counts = best_action_counts.mean(axis=1)

    return mean_rewards, mean_best_action_counts

# Parameters
k = 10  # Number of arms
epsilons = [0.1, 0.01]  # Exploration rates
runs = 2000  # Number of runs
time = 1000  # Number of steps

bandits = [MultiArmedBandit(k, epsilon) for epsilon in epsilons]
mean_rewards, mean_best_action_counts = simulate(runs, time, bandits)

# Plotting
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
for epsilon, rewards in zip(epsilons, mean_rewards):
    plt.plot(rewards, label=f'ε = {epsilon}')
plt.xlabel('Steps')
plt.ylabel('Average reward')
plt.legend()

plt.subplot(1, 2, 2)
for epsilon, counts in zip(epsilons, mean_best_action_counts):
    plt.plot(counts, label=f'ε = {epsilon}')
plt.xlabel('Steps')
plt.ylabel('% Optimal action')
plt.legend()

plt.tight_layout()
plt.show()