{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class OffPolicyMonteCarloControl:\n",
        "    def __init__(self, num_actions, epsilon=0.1, gamma=1.0):\n",
        "        self.num_actions = num_actions\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = gamma\n",
        "        self.Q = np.zeros((num_actions,))\n",
        "\n",
        "    def epsilon_greedy_policy(self, state):\n",
        "        return np.random.randint(self.num_actions) if np.random.rand() < self.epsilon else np.argmax(self.Q)\n",
        "\n",
        "    def update_Q(self, episode):\n",
        "        G, W = 0, 1\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = self.gamma * G + reward\n",
        "            self.Q[action] += W / (self.num_actions * self.epsilon + (1 - self.epsilon) * np.sum(self.Q))\n",
        "            if action != np.argmax(self.Q):\n",
        "                break\n",
        "            W *= 1 / (self.epsilon if action == np.argmax(self.Q[state]) else self.num_actions)\n",
        "\n",
        "    def learn(self, episodes, behavior_policy):\n",
        "        for _ in range(episodes):\n",
        "            episode = []\n",
        "            state = 0\n",
        "            while True:\n",
        "                action = behavior_policy(state)\n",
        "                next_state = np.random.choice(self.num_actions)\n",
        "                reward = 1 if next_state == self.num_actions - 1 else 0\n",
        "                episode.append((state, action, reward))\n",
        "                if next_state == self.num_actions - 1:\n",
        "                    break\n",
        "                state = next_state\n",
        "            self.update_Q(episode)\n",
        "\n",
        "# Example usage:\n",
        "num_actions = 4\n",
        "num_episodes = 1000\n",
        "epsilon = 0.1\n",
        "gamma = 1.0\n",
        "\n",
        "def behavior_policy(state):\n",
        "    return np.random.randint(num_actions) if np.random.rand() < epsilon else np.argmax(Q)\n",
        "\n",
        "agent = OffPolicyMonteCarloControl(num_actions, epsilon, gamma)\n",
        "agent.learn(num_episodes, behavior_policy)\n",
        "optimal_policy = np.argmax(agent.Q)\n",
        "print(\"Optimal Policy:\", optimal_policy)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbOme7POvH8A",
        "outputId": "5d3056c0-520f-4282-eb5c-baceca62026c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S0hrvcHHU1E9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}